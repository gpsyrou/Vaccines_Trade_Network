{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaccines Trade Network\n",
    "### Author: Georgios Spyrou\n",
    "### Date: 23/05/2020\n",
    "\n",
    "<img src=\"https://www.our-voices.org.uk/assets/images/Network-diagram.png\" width=720 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Puprose\n",
    "Scope of this project is to construct the global trade network of vaccines around the globe for multiple years(current data span from 2017 to 2019. Through plotting and statistical analysis, we want to identify informative patterns on how different countries change their import/export activity of vaccines. \n",
    "\n",
    "### Part 1: Data Retrieval\n",
    "In order to find and retrieve data we have utilised the UN Comtrade API (https://comtrade.un.org/Data/).This is a great place if someone wants to find data regarding exports/impors of countries around the world, pretty much for any product/service. The data are very well documented and visualized on the website, and it's easy to do some exploration  and locate data that seem interesting for your project.\n",
    "\n",
    "Therefore, after I managed to identify the relevant data that I wanted to work with (vaccines for human medicine), I had to find a way to leverage the API to get my data. Even though the website allows us to download sample CSV files, if we wanted to do that for multiple years and countries it would take a lot of time. Hence, I have decided to approach the data retrival from a Python standpoint, in order to automate this task.\n",
    "\n",
    "Before we jump to the part of how we are going to automate the data retrieval, it might worths it to explain what exactly we are aiming to retrieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UN Comtrade database is giving us the opportunity to pick from a plethora of goods and services. My first task was to find the specific code that corresponds to human vaccines. After a bit of research we have found that this code is *300220*, which is a unique value that will allow us to only pick data for this good/product. After we located our product code, we have to decide on the time range that we want to pick data for. This can quickly get tricky as the database does not allow you to many years/countries all at once. But for now lets say that the scope of interest was *monthly* data from 2017 to 2019, for as many countries as possible - mainly because there are countries which do not seem to import/export vaccines or we do not have relevant information about them.\n",
    "\n",
    "Now that are know pretty much what we are looking for, lets dive into creating some API calls in Python to automatically get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Setting up the parameters for the API calls to receive the data\n",
    "\n",
    "max_rec = 100000\n",
    "output_fmt = 'csv'\n",
    "trade_type = 'C'            # Commodities\n",
    "frequency = 'M'             # Monthly\n",
    "px = 'HS'                   # Classification for products\n",
    "cc = 300220                 # Subcategory --> 300220 code for Vaccines\n",
    "reporter = 'all'\n",
    "partner = 'all'                 \n",
    "rg ='all'\n",
    "\n",
    "# Connection string to comtrade.un.org based on the parameters above\n",
    "api_call_string = f'http://comtrade.un.org/api/get?max={max_rec}&type={trade_type}&freq={frequency}&px={px}&ps=year&r=reporter&p={partner}&rg={rg}&cc={cc}&fmt={output_fmt}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have set up the string that will be used to do the appropriate API calls, it's time create a function that will use this string to retrieve the data and generate the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataCall(api_string: str, reporterid: str, reportername: str, year: int, out_folder: str) -> None:\n",
    "    '''\n",
    "    Create a .csv file that contains the data as received from  https://comtrade.un.org/Data/, for a specific year.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        api_string: String that contains the URL for the API call. The string already contains all the paremeters required for the call.\n",
    "        reporter: Specify Reporter country.\n",
    "        year: Specify year of interest.\n",
    "    Returns:\n",
    "    -------\n",
    "        None: The output is a .csv file that contains the data for a specified year.\n",
    "    '''\n",
    "    csv_by_year_out_loc = os.path.join(out_folder, f'{year}')\n",
    "    if not os.path.exists(csv_by_year_out_loc):\n",
    "        os.makedirs(name=csv_by_year_out_loc)\n",
    "\n",
    "    api_string = api_string.replace('year', f'{year}').replace('reporter', f'{reporterid}')\n",
    "    print(api_string)\n",
    "\n",
    "    response = requests.get(url=api_string)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print('Could not access the API!')\n",
    "    else:\n",
    "        decoded_data = response.content.decode('utf-8')\n",
    "        csv_file = csv.reader(decoded_data.splitlines())\n",
    "        datalines = list(csv_file)\n",
    "\n",
    "        with open(os.path.join(csv_by_year_out_loc, f'Comtrade_Vaccines_Data_{reportername}_{year}.csv'), 'w', newline='') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerows(datalines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this call will run for *each* country, therefore we will end up with a lot of csv files. The country codes can be found here: https://comtrade.un.org/Data/cache/reporterAreas.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters_url = 'https://comtrade.un.org/Data/cache/partnerAreas.json'\n",
    "reporters_resp = requests.get(url=reporters_url)\n",
    "json_data = json.loads(reporters_resp.text)\n",
    "\n",
    "reporters_list = [rep for rep in json_data['results']]\n",
    "\n",
    "# Get the data as separate csv files, each for every year of interest\n",
    "years_ls = [2017]\n",
    "outputFilesFolder = f'CSVFiles\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api_check, repd in enumerate(reporters_list):\n",
    "    # Need to make the script to sleep every 100 calls, as the API is blocking us for an hour for every 100 calls.\n",
    "    if api_check !=0 and api_check % 100 == 0:\n",
    "        time.sleep(3600)\n",
    "    countryname = repd['text']\n",
    "    c_id = repd['id']\n",
    "    print(f'\\nCountry..: {countryname}')\n",
    "    for year in years_ls:\n",
    "        print(f'\\nReceiving the data for {year} from https://comtrade.un.org/...\\n')\n",
    "        getDataCall(api_call_string, reporterid=c_id, reportername=countryname, year=year, out_folder=outputFilesFolder)\n",
    "        time.sleep(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have successfully get the data for every country that we cound find a corresponding country code. That said, as we mentioned before there are some countries that contain no data for a specific year - which is might be for many reasons but this is out of the scope of this project. Hence, after we have ended up with multiple csv files , one for each country , we can repeat this process for as many years as we want to. I have repeated it three times (2017, 2018, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably imagine this process generated over 300 csv files (3 years x over 100 countries). Hence it was necessary to perform some data cleaning operations in the directory that the files were created. The main task was to merge all this files for a specific year (e.g. all countries for 2017 merged to a common csv file), and at the same time delete all the files from the directory that contained no data.\n",
    "\n",
    "In order to do that we have created a script to automatically do this for us, but as it might not be that of an interesting task we are going to leave it outside of this notebook. That said, the code that completes this can be found here: https://github.com/gpsyrou/Vaccines_Trade_Network/blob/master/dataCleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
