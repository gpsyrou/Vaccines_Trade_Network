{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaccines Trade Network: Analysis and Forecasting with ARIMA and Recurrent Neural Networks\n",
    "### Author: Georgios Spyrou\n",
    "### Date: 23/05/2020\n",
    "\n",
    "<img src=\"https://www.our-voices.org.uk/assets/images/Network-diagram.png\" width=720 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Puprose\n",
    "\n",
    "Scope of this project is to construct the global trade network of vaccines around the globe for multiple years(current data span from 2017 to 2019. Through plotting and statistical analysis, we want to identify informative patterns on how different countries change their import/export activity of vaccines. \n",
    "\n",
    "## Part 1: Data Retrieval\n",
    "\n",
    "In order to find and retrieve data we have utilised the UN Comtrade API (https://comtrade.un.org/Data/).This is a great place if someone wants to find data regarding exports/impors of countries around the world, pretty much for any product/service. The data are very well documented and visualized on the website, and it's easy to do some exploration  and locate data that seem interesting for your project.\n",
    "\n",
    "Therefore, after I managed to identify the relevant data that I wanted to work with (vaccines for human medicine), I had to find a way to leverage the API to get my data. Even though the website allows us to download sample CSV files, if we wanted to do that for multiple years and countries it would take a lot of time. Hence, I have decided to approach the data retrival from a Python standpoint, in order to automate this task.\n",
    "\n",
    "Before we jump to the part of how we are going to automate the data retrieval, it might worths it to explain what exactly we are aiming to retrieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UN Comtrade database is giving us the opportunity to pick from a plethora of goods and services. My first task was to find the specific code that corresponds to human vaccines. After a bit of research we have found that this code is *300220*, which is a unique value that will allow us to only pick data for this good/product. After we located our product code, we have to decide on the time range that we want to pick data for. This can quickly get tricky as the database does not allow you to many years/countries all at once. But for now lets say that the scope of interest was *monthly* data from 2017 to 2019, for as many countries as possible - mainly because there are countries which do not seem to import/export vaccines or we do not have relevant information about them.\n",
    "\n",
    "Now that are know pretty much what we are looking for, lets dive into creating some API calls in Python to automatically get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Setting up the parameters for the API calls to receive the data\n",
    "\n",
    "max_rec = 100000\n",
    "output_fmt = 'csv'\n",
    "trade_type = 'C'            # Commodities\n",
    "frequency = 'M'             # Monthly\n",
    "px = 'HS'                   # Classification for products\n",
    "cc = 300220                 # Subcategory --> 300220 code for Vaccines\n",
    "reporter = 'all'\n",
    "partner = 'all'                 \n",
    "rg ='all'\n",
    "\n",
    "# Connection string to comtrade.un.org based on the parameters above\n",
    "api_call_string = f'http://comtrade.un.org/api/get?max={max_rec}&type={trade_type}&freq={frequency}&px={px}&ps=year&r=reporter&p={partner}&rg={rg}&cc={cc}&fmt={output_fmt}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have set up the string that will be used to do the appropriate API calls, it's time create a function that will use this string to retrieve the data and generate the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataCall(api_string: str, reporterid: str, reportername: str, year: int, out_folder: str) -> None:\n",
    "    '''\n",
    "    Create a .csv file that contains the data as received from  https://comtrade.un.org/Data/, for a specific year.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        api_string: String that contains the URL for the API call. The string already contains all the paremeters required for the call.\n",
    "        reporter: Specify Reporter country.\n",
    "        year: Specify year of interest.\n",
    "    Returns:\n",
    "    -------\n",
    "        None: The output is a .csv file that contains the data for a specified year.\n",
    "    '''\n",
    "    csv_by_year_out_loc = os.path.join(out_folder, f'{year}')\n",
    "    if not os.path.exists(csv_by_year_out_loc):\n",
    "        os.makedirs(name=csv_by_year_out_loc)\n",
    "\n",
    "    api_string = api_string.replace('year', f'{year}').replace('reporter', f'{reporterid}')\n",
    "    print(api_string)\n",
    "\n",
    "    response = requests.get(url=api_string)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print('Could not access the API!')\n",
    "    else:\n",
    "        decoded_data = response.content.decode('utf-8')\n",
    "        csv_file = csv.reader(decoded_data.splitlines())\n",
    "        datalines = list(csv_file)\n",
    "\n",
    "        with open(os.path.join(csv_by_year_out_loc, f'Comtrade_Vaccines_Data_{reportername}_{year}.csv'), 'w', newline='') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerows(datalines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this call will run for *each* country, therefore we will end up with a lot of csv files. The country codes can be found here: https://comtrade.un.org/Data/cache/reporterAreas.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters_url = 'https://comtrade.un.org/Data/cache/partnerAreas.json'\n",
    "reporters_resp = requests.get(url=reporters_url)\n",
    "json_data = json.loads(reporters_resp.text)\n",
    "\n",
    "reporters_list = [rep for rep in json_data['results']]\n",
    "\n",
    "# Get the data as separate csv files, each for every year of interest\n",
    "years_ls = [2017]\n",
    "outputFilesFolder = f'CSVFiles\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api_check, repd in enumerate(reporters_list):\n",
    "    # Need to make the script to sleep every 100 calls, as the API is blocking us for an hour for every 100 calls.\n",
    "    if api_check !=0 and api_check % 100 == 0:\n",
    "        time.sleep(3600)\n",
    "    countryname = repd['text']\n",
    "    c_id = repd['id']\n",
    "    print(f'\\nCountry..: {countryname}')\n",
    "    for year in years_ls:\n",
    "        print(f'\\nReceiving the data for {year} from https://comtrade.un.org/...\\n')\n",
    "        getDataCall(api_call_string, reporterid=c_id, reportername=countryname, year=year, out_folder=outputFilesFolder)\n",
    "        time.sleep(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have successfully get the data for every country that we cound find a corresponding country code. That said, as we mentioned before there are some countries that contain no data for a specific year - which is might be for many reasons but this is out of the scope of this project. Hence, after we have ended up with multiple csv files , one for each country , we can repeat this process for as many years as we want to. I have repeated it three times (2017, 2018, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably imagine this process generated over 300 csv files (3 years x over 100 countries). Hence it was necessary to perform some data cleaning operations in the directory that the files were created. The main task was to merge all this files for a specific year (e.g. all countries for 2017 merged to a common csv file), and at the same time delete all the files from the directory that contained no data.\n",
    "\n",
    "In order to do that we have created a script to automatically do this for us, but as it might not be that of an interesting task we are going to leave it outside of this notebook. That said, the code that completes this can be found here: https://github.com/gpsyrou/Vaccines_Trade_Network/blob/master/dataCleaning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data organized into separate csv files by year, we can start the task of exploring what we actually got from the data retriaval process. We will explore this by taking into consideration all the data from 2017 to 2019.\n",
    "\n",
    "**Note**: Frow now on you are going to see two packages, one named Functions and one named VaccinesTradeNetworkClass. Please note that these are custom packages that I have created for the purposes of this project. The first one contains some functions that we will keep using for our EDA tasks, while the VaccinesTradeNetworkClass one will be used later on, when we jump into creating out network graph objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "project_dir = 'C:\\\\Users\\\\george\\\\Desktop\\\\GitHub\\\\Projects\\\\Comtrade_Network'\n",
    "os.chdir(project_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom packages\n",
    "from Functions import tradeNetworkFunctions as tnf\n",
    "from VaccinesTradeNetworkClass import VaccinesTradeNetwork\n",
    "\n",
    "# Plotting and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "csv_files_loc = os.path.join(project_dir, 'Merged_CSVs')\n",
    "maindf = pd.concat([pd.read_csv(os.path.join(csv_files_loc, file)) for file in os.listdir(csv_files_loc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we need to explore the dataset and identify any potential issues in the data tha require cleaning, and also make sure that we understand the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = maindf.describe()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the summary, few of the features does not seem to provide useful information for our analysis, as they either contain only empty values (e.g. 'Qty') or they contain fixed values (e.g.'Aggregate Level'). Therefore we will exclude the useless columns from our dataset to reduce the noise and the dimensions of our feature space.\n",
    "\n",
    "Note: More information regarding the feautures can be found here: https://comtrade.un.org/data/MethodologyGuideforComtradePlus.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features_ls = ['Period', 'Reporter Code', 'Reporter', 'Partner Code',\n",
    "                      'Partner', 'Trade Flow', 'Commodity', 'Netweight (kg)',\n",
    "                      'Trade Value (US$)']\n",
    "df = maindf[useful_features_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Trade Flow'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider both 'Re-imports' and 'Re-exports' as 'Imports' and 'Exports' respectively and we will drop the entries where we dont have info about the trade flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_flow_dict = {'Re-imports':'Imports', \n",
    "                   'Re-exports':'Exports',\n",
    "                   'Imports':'Imports', \n",
    "                   'Exports':'Exports'}\n",
    "\n",
    "df['Trade Flow'] = df['Trade Flow'].map(trade_flow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Partner.unique()\n",
    "\n",
    "df['Partner'].replace(\n",
    "    to_replace='United States of America',\n",
    "    value='USA',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df['Reporter'].replace(\n",
    "    to_replace='United States of America',\n",
    "    value='USA',\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that we have a node called 'World' but we would like to  analyze the trade relationships between specific countries. Thus we will exclude from the analysis the cases where the reporter or partner is 'World'. Finally, **Period** will be the column that identifies the specific datetime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Partner != 'World']\n",
    "\n",
    "df['Period'] = pd.to_datetime(df['Period'], format='%Y%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except the nodes of our analysis which will correspond to countries, the other main features of interest are the **Netweight** of the export/import in kilograms as well as the **Trade Value in US dollars($)**.\n",
    "\n",
    "Now we are in a position that we can start doing some data visualization to get a better understanding of our dataset. Therefore,l ets find the top countries that import/export vaccines in terms of US dollars($)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the top-n number of countries that we want to plot for\n",
    "topn = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point its necessary to introduce two functions for our EDA analysis. One function is being used in order to calculate the statistics of interest, while the second one is to plot the results. Both of these functions our packaged in *Functions* as we mentioned above, but as they mighe be interesting we are also going to include them into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAggStatistics(df: pd.core.frame.DataFrame, feature: str,\n",
    "                     kind: str, year: str) -> pd.core.frame.DataFrame:\n",
    "    '''\n",
    "    Given a dataframe and a feature column (numerical), identify the top\n",
    "    importers/exporters.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        df: DataFrame that contains the data and the required features.\n",
    "        feature: Numerical feature to aggregate (e.g. 'Trade Value (US$)', 'Netweight (kg)')\n",
    "        kind: 'Imports', 'Exports'\n",
    "        year: Specify year of interest or 'all' for all years.\n",
    "    Returns:\n",
    "    -------\n",
    "        df_sorted: Sorted dataframe that contains the aggregated values.\n",
    "    '''\n",
    "    if year == 'all':\n",
    "        df = df.loc[df['Trade Flow'] == kind, [feature,\n",
    "            'Reporter']].groupby(['Reporter']).agg(['sum']).reset_index()\n",
    "        df['Year'] = int(year)\n",
    "    else:\n",
    "        df = df.loc[(df['Trade Flow'] == kind) & (df['Period'] > f'{year}-01-01') & (df['Period'] <= f'{year}-12-31'), \n",
    "                    [feature,'Reporter']].groupby(['Reporter']).agg(['sum']).reset_index()\n",
    "        df['Year'] = int(year)\n",
    "    df_sorted = df.sort_values(by=(feature,'sum'), ascending=False)\n",
    "    \n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTopnCountries(df: pd.core.frame.DataFrame, feature: str,\n",
    "                      topn: int, kind: str, year: str) -> None:\n",
    "    '''\n",
    "    Create a bar plot of the top-N countries compared to an aggregated column.        \n",
    "    '''\n",
    "    if kind != 'Import' and kind != 'Export':\n",
    "        raise ValueError('Trade flow is not set to Import or Export')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    g = sns.barplot(x='Reporter', y=(feature,'sum'), data=df[0:topn],\n",
    "                    palette='muted')\n",
    "\n",
    "    if topn > 5 and topn <= 10:\n",
    "        rot = 0\n",
    "    elif topn > 10:\n",
    "        rot = 75\n",
    "    else:\n",
    "        rot = 0\n",
    "\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=rot)\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    if year == 'all':\n",
    "        plt.title(f'Top-{topn} {kind}ers of vaccines around the globe')\n",
    "    else:\n",
    "        plt.title(f'Top-{topn} {kind}ers of vaccines around the globe in {year}')\n",
    "    plt.xlabel(f'{kind}er Country')\n",
    "    if feature == 'Trade Value (US$)':\n",
    "        plt.ylabel(f'Total amount of {kind}s in US$')\n",
    "    else:\n",
    "        plt.ylabel(f'Total amount of {kind}s in Netweight (kg)')\n",
    "    plt.grid(True, alpha = 0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore which are the top importers and exporters of vaccines for the different years of interest.\n",
    "\n",
    "The analysis will be focused on Trade Value in US($) dollars, but it can be easily applied to the Netweight of exports/imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if we want to focus on a specific year (e.g. '2018') or 'all'\n",
    "\n",
    "year = '2017'\n",
    "\n",
    "# Trade Value\n",
    "top_importers_2017 = getAggStatistics(df, feature='Trade Value (US$)',\n",
    "                                     kind='Imports', year=year)\n",
    "top_importers_2017[0:topn]\n",
    "\n",
    "\n",
    "year = '2018'\n",
    "\n",
    "# Trade Value\n",
    "top_importers_2018 = getAggStatistics(df, feature='Trade Value (US$)',\n",
    "                                     kind='Imports', year=year)\n",
    "top_importers_2018[0:topn]\n",
    "\n",
    "\n",
    "year = '2019'\n",
    "\n",
    "# Trade Value\n",
    "top_importers_2019 = getAggStatistics(df, feature='Trade Value (US$)',\n",
    "                                     kind='Imports', year=year)\n",
    "top_importers_2019[0:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopnCountries(df=top_importers_2019, feature='Trade Value (US$)',\n",
    "                      topn=topn, kind='Import', year=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its interesting that except the firt three countries (Belgium, USA, UK), the rest of the countries seem to import much less compared with the top-3 importers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Main analysis (Times Series with ARIMA & Recurrent Neural Networks)\n",
    "\n",
    "In this part we are going to focus only at United Kingdom as our base country and perform an in depth analysis about the countries that UKise trading with, attempting to identify interesting patterns in the results. This is going to be done in two different parts. In the first part we will focus in the variations of imports by month, i.e. we will create Time Series object for the base country, where each observation will be the the Total Value of imports in a specific time point. The second part will focus on Network Analysis.\n",
    "\n",
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are creating the objects that will contain all the information that we need for our analysis. Each object is going to represent a country and in our analysis, as we mentioned previously, we will focus on United Kingdom - but we will attempt to make the codebase as robust as possible, so that the same analysis can be easily performed for any other country. \n",
    "\n",
    "At this point we will leverage the class **VaccinesTradeNetwork** which is a class that will allow us to create our analysis without having to include the code snipets in the notebook. If you want more information regarding how this class is organized, feel free to have a look at the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object for United Kingdom \n",
    "united_kingdom = VaccinesTradeNetwork(df, country='United Kingdom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are focusing in the cases that the UK is our base country and take into consideration all data that 1) UK is the importer country of vaccines or 2) other countries exporting **to** UK (and thus that makes UK again the importer country). This has been implemented in the createFlowDF method, in the VaccinesTradeNetwork **VaccinesTradeNetwork** class. If you would like to change our analysis to exports, we could just change the _tradeflow_ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_kingdom_imports_df = united_kingdom.createFlowDF(tradeflow='Imports',\n",
    "                                                        source='Reporter',\n",
    "                                                        target='Partner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_kingdom_ts = united_kingdom.generateTimeSeries(partner_country='USA', timeframe='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.facecolor'] = 'whitesmoke'\n",
    "united_kingdom.plotTimeSeries(partner_list=['USA'], timeframe='month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1.1\n",
    "\n",
    "Our forecasting task will be separated into two parts. We will implement an ARIMA model to predict our values for 2019, based on the values that we have for the period 2010-2018. The second part will have the same goal but this time we will construct a Recurrent Neural Network (RNN) to make the predictions. Finally, we will compare the outputs of both methods and see which one is better for our forecasting task.\n",
    "\n",
    "### Fitting an ARIMA model\n",
    "\n",
    "At this stage we are going to fit an _ARIMA_ (Autoregressive Integrated Moving Average) model to forecast the 2019 Trade Value of UK imports of Vaccines from the USA.\n",
    "\n",
    "\n",
    "ARIMA(p,d,q) is a statistical method(model) for Time Series analysis, especially around forecasting future points of a series of observations or just getting a better understanding of our data (e.g. a trend in the data).\n",
    "\n",
    "The main components of an ARIMA model, are:\n",
    "    \n",
    " **1.**   AR (Autoregression): This means that the model is leveraging the dependent relationship between an observation and some number of lagged observations (i.e. we are regressing the variable of interest on its on prior values).\n",
    " \n",
    " **2.**   I (Integrated): This is a common step to make the Time Series stationary, where we are subtracting an observation from an observation at the previous time step).\n",
    " \n",
    " **3.**   MA (Moving Average): The specification indicating that the predictor variable depends linearly on current/past observations. \n",
    " \n",
    " **4.**   p: Number of lag observations in the model (lag order).\n",
    " \n",
    " **5.**   d: Number of times we subtracted past values from the data (degree of differencing).\n",
    " \n",
    " **6.**   q: Order of the moving average window (moving average).\n",
    " \n",
    "In the next few steps our task is to identify the values of parameters p,d,q from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot, lag_plot\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we start building the model we should make sure that we are not violating any assumptions regarding ARIMA models. One of these assumptions is that our Time Series is stationary (i.e. the mean and the variance remains constant over time).\n",
    "\n",
    "By looking at Figure 1.1 we can see that the time series in our case is not stationary and we have a decreasing trend, along with seasonal trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At first we look at the autocorrelation plot of the time series. This plot helps us identify a statistically significant amount of lags for the time series (i.e the lag order). This can be identified by looking at which point our series is between the dashes, which is the significant range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "autocorrelation_plot(united_kingdom_ts['Trade Value (US$)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the plot we can infer that there is a positive correlation between lags 1-30, but it seems to be significant at the first 1 to 18 lags.\n",
    "Thus a good starting point would be at 12 lags, which correspond to window of an observation lagged to its value at the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_order = 12\n",
    "\n",
    "# Create a Lag plot\n",
    "tnf.create_lag_plot(united_kingdom_ts['Trade Value (US$)'], lag = lag_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lag plot is a special type of scatter plot with the two variables (X,Y) “lagged”.\n",
    "\n",
    "A “lag” is a fixed amount of passing time; One set of observations in a time series is plotted (lagged) against a second, later set of data. \n",
    "The kth lag is the time period that happened “k” time points before time i.\n",
    "(reference: https://www.statisticshowto.datasciencecentral.com/lag-plot/)\n",
    "\n",
    "In our case we can see that we have a positive correlation and there is a linear pattern (suggesting autocorellation) between time t and t-k where k = lag order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check and evaluate the time series for stationarity we will be using a custom functions that is using the Dickey-Fuller test.In this test, our null hypothesis is that the Time Series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnf.stationarity_checking(df=united_kingdom_ts['Trade Value (US$)'], window=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our time series is not stationary yet, as the test statistic is more that the critical values. Therefore we fail to reject the null hypothesis.\n",
    "\n",
    "The reason why this time series is not stationary lies to the fact that we have a clear trend and seasonality in our data. One of the most common ways to resolve this issue is by differencing (point 2 at ARIMA model explanation above).Hence, when it's time to run the ARIMA model we have to make sure to include an **'I'** (integrated) parameter - which will give us the value of the 'd' paremeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define the values for 'p' and 'q', we will use ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function). The ACF is helping us to measure the correlation of our time series with a lagged version of itself, for a specific lag. The PACF is similar to ACF but we are also excluding the variations explained already between an observation and its lagged value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Autocorrelation Plot\n",
    "tnf.plot_acf_pacf(united_kingdom_ts['Trade Value (US$)'], lag=lag_order, kind='pacf')\n",
    "\n",
    "# Autocorrelation plot\n",
    "tnf.plot_acf_pacf(united_kingdom_ts['Trade Value (US$)'], lag=lag_order, kind='acf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF and PACF plots indicating that we have clear statistical significance for lags 1 or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split our dataset into train and test sets, where the training set will contain the data for period 2010-2018, and the test set will contain data for 2019. Our task will be - base on the training data - how well we can predict the values for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tnf.split_test_train(united_kingdom_ts['Trade Value (US$)'], num_months_test=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test sets\n",
    "plt.figure(figsize=(16,8))\n",
    "train.plot(marker = '.', color = 'blue', label = 'Train')\n",
    "test.plot(marker = '.', color = 'red', label = 'Test')\n",
    "plt.legend(loc = 'best', shadow = True, fontsize = 'x-large')\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ARIMA model\n",
    "p = lag_order\n",
    "d = 1\n",
    "q = 0\n",
    "\n",
    "predicted = []\n",
    "series_updated = [x for x in train]\n",
    "\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(series_updated, order = (p,d,q))\n",
    "    model_output = model.fit(disp=0)\n",
    "    pred = model_output.forecast()\n",
    "    yhat = pred[0]\n",
    "    predicted.append(yhat)\n",
    "    obs = test[t]\n",
    "    series_updated.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predictions visually\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(test, marker='.', color= 'blue', label='True')\n",
    "plt.plot(pd.Series(predicted, index=test.index), marker='.', color='red', label='Predicted')\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.legend()\n",
    "\n",
    "# Compute the Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rms = np.sqrt(mean_squared_error(np.array(test), predicted))\n",
    "print('RMSE: {0}'.format(rms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). \n",
    "Residuals are a measure of how far from the regression line data points are; \n",
    "RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. \n",
    "Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residual errors\n",
    "resid = pd.DataFrame(model_fit.resid)\n",
    "resid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density of the residuals\n",
    "resid.plot(kind='kde')\n",
    "print(resid.describe())\n",
    "# We can see that there is a slight bias in the prediction as there is non-zero mean\n",
    "# in the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Recurrent Neural Network (RNN)\n",
    "\n",
    "In the case of Univariate Time Series, like the one we are facing in our problem, before we create the RNN architecture we have to complete some data preparation. Specifically, our task is to make the RNN to learn a function that it will take as input a series of observations (past steps) to predict the next observation(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general is a good practice before we work with Neural Networks to normalize the data. In that case we are going to scale our data to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_uk_to_usa = united_kingdom_ts['Trade Value (US$)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train is (108,)\n",
      "Shape of test is (12,)\n"
     ]
    }
   ],
   "source": [
    "# Split the series into train and test sets\n",
    "# parameter num_months_test defines how many months we take as test data\n",
    "train, test = tnf.split_test_train(time_series_uk_to_usa, num_months_test=12)\n",
    "print(f'Shape of train is {train.shape}\\nShape of test is {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\george\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "train_scaled = pd.Series(np.concatenate(train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_past = 3\n",
    "n_steps_future = 1\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is (105, 3)\n",
      "Shape of y is (105, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = tnf.split_into_samples(train_scaled, n_steps_past=n_steps_past, n_steps_future=n_steps_future)\n",
    "print(f'Shape of X is {X.shape}\\nShape of y is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape((X.shape[0], n_steps_past, n_features))\n",
    "X.shape\n",
    "X[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.0782\n",
      "Epoch 2/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0739\n",
      "Epoch 3/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0698\n",
      "Epoch 4/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0664\n",
      "Epoch 5/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0630\n",
      "Epoch 6/300\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0596\n",
      "Epoch 7/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0562\n",
      "Epoch 8/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0535\n",
      "Epoch 9/300\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0504\n",
      "Epoch 10/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0483\n",
      "Epoch 11/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0457\n",
      "Epoch 12/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0436\n",
      "Epoch 13/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0415\n",
      "Epoch 14/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0399\n",
      "Epoch 15/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0379\n",
      "Epoch 16/300\n",
      "105/105 [==============================] - 0s 180us/step - loss: 0.0367\n",
      "Epoch 17/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0353\n",
      "Epoch 18/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0344\n",
      "Epoch 19/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0335\n",
      "Epoch 20/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0328\n",
      "Epoch 21/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0322\n",
      "Epoch 22/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0317\n",
      "Epoch 23/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0313\n",
      "Epoch 24/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0308\n",
      "Epoch 25/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0302\n",
      "Epoch 26/300\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0298\n",
      "Epoch 27/300\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0293\n",
      "Epoch 28/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0287\n",
      "Epoch 29/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0282\n",
      "Epoch 30/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0276\n",
      "Epoch 31/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0270\n",
      "Epoch 32/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0266\n",
      "Epoch 33/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0262\n",
      "Epoch 34/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0256\n",
      "Epoch 35/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0252\n",
      "Epoch 36/300\n",
      "105/105 [==============================] - 0s 180us/step - loss: 0.0248\n",
      "Epoch 37/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0243\n",
      "Epoch 38/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0239\n",
      "Epoch 39/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0236\n",
      "Epoch 40/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0233\n",
      "Epoch 41/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0230\n",
      "Epoch 42/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0228\n",
      "Epoch 43/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0226\n",
      "Epoch 44/300\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0224\n",
      "Epoch 45/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0221\n",
      "Epoch 46/300\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0221\n",
      "Epoch 47/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0218\n",
      "Epoch 48/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0217\n",
      "Epoch 49/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0217\n",
      "Epoch 50/300\n",
      "105/105 [==============================] - 0s 180us/step - loss: 0.0215\n",
      "Epoch 51/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0215\n",
      "Epoch 52/300\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.0214\n",
      "Epoch 53/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0214\n",
      "Epoch 54/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0214\n",
      "Epoch 55/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0214\n",
      "Epoch 56/300\n",
      "105/105 [==============================] - 0s 157us/step - loss: 0.0213\n",
      "Epoch 57/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0212\n",
      "Epoch 58/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0212\n",
      "Epoch 59/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0211\n",
      "Epoch 60/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 61/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 62/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 63/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 64/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 65/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0212\n",
      "Epoch 66/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0211\n",
      "Epoch 67/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0210\n",
      "Epoch 68/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0210\n",
      "Epoch 69/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0210\n",
      "Epoch 70/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0211\n",
      "Epoch 71/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0211\n",
      "Epoch 72/300\n",
      "105/105 [==============================] - 0s 114us/step - loss: 0.0211\n",
      "Epoch 73/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0210\n",
      "Epoch 74/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0210\n",
      "Epoch 75/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0210\n",
      "Epoch 76/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0210\n",
      "Epoch 77/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0210\n",
      "Epoch 78/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0209\n",
      "Epoch 79/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0210\n",
      "Epoch 80/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0210\n",
      "Epoch 81/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0209\n",
      "Epoch 82/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0209\n",
      "Epoch 83/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0209\n",
      "Epoch 84/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0208\n",
      "Epoch 85/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0209\n",
      "Epoch 86/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0209\n",
      "Epoch 87/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0209\n",
      "Epoch 88/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0209\n",
      "Epoch 89/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 90/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 91/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0209\n",
      "Epoch 92/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 93/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 94/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0209\n",
      "Epoch 95/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0208\n",
      "Epoch 96/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0208\n",
      "Epoch 97/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 142us/step - loss: 0.0208\n",
      "Epoch 98/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 99/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0208\n",
      "Epoch 100/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0208\n",
      "Epoch 101/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0208\n",
      "Epoch 102/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0208\n",
      "Epoch 103/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0208\n",
      "Epoch 104/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0208\n",
      "Epoch 105/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0207\n",
      "Epoch 106/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0207\n",
      "Epoch 107/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0208\n",
      "Epoch 108/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0207\n",
      "Epoch 109/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0207\n",
      "Epoch 110/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0208\n",
      "Epoch 111/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0207\n",
      "Epoch 112/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0206\n",
      "Epoch 113/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0206\n",
      "Epoch 114/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0206\n",
      "Epoch 115/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0206\n",
      "Epoch 116/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0206\n",
      "Epoch 117/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0207\n",
      "Epoch 118/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0207\n",
      "Epoch 119/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0207\n",
      "Epoch 120/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0207\n",
      "Epoch 121/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0206\n",
      "Epoch 122/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0206\n",
      "Epoch 123/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0206\n",
      "Epoch 124/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 125/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 126/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 127/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0205\n",
      "Epoch 128/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0205\n",
      "Epoch 129/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 130/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 131/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 132/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 133/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0204\n",
      "Epoch 134/300\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.037 - 0s 161us/step - loss: 0.0204\n",
      "Epoch 135/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0204\n",
      "Epoch 136/300\n",
      "105/105 [==============================] - 0s 149us/step - loss: 0.0204\n",
      "Epoch 137/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0204\n",
      "Epoch 138/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0204\n",
      "Epoch 139/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0204\n",
      "Epoch 140/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0204\n",
      "Epoch 141/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 142/300\n",
      "105/105 [==============================] - 0s 139us/step - loss: 0.0204\n",
      "Epoch 143/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0204\n",
      "Epoch 144/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0205\n",
      "Epoch 145/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0205\n",
      "Epoch 146/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0204\n",
      "Epoch 147/300\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.0204\n",
      "Epoch 148/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0204\n",
      "Epoch 149/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0203\n",
      "Epoch 150/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0203\n",
      "Epoch 151/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0203\n",
      "Epoch 152/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0203\n",
      "Epoch 153/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0203\n",
      "Epoch 154/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0204\n",
      "Epoch 155/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0204\n",
      "Epoch 156/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0203\n",
      "Epoch 157/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0204\n",
      "Epoch 158/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0203\n",
      "Epoch 159/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0203\n",
      "Epoch 160/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0204\n",
      "Epoch 161/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0202\n",
      "Epoch 162/300\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.025 - 0s 133us/step - loss: 0.0202\n",
      "Epoch 163/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0202\n",
      "Epoch 164/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0203\n",
      "Epoch 165/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0203\n",
      "Epoch 166/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0202\n",
      "Epoch 167/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0203\n",
      "Epoch 168/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0202\n",
      "Epoch 169/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0202\n",
      "Epoch 170/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0202\n",
      "Epoch 171/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0201\n",
      "Epoch 172/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0202\n",
      "Epoch 173/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0201\n",
      "Epoch 174/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0202\n",
      "Epoch 175/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0202\n",
      "Epoch 176/300\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0201\n",
      "Epoch 177/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0201\n",
      "Epoch 178/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0201\n",
      "Epoch 179/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0201\n",
      "Epoch 180/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0201\n",
      "Epoch 181/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 182/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0201\n",
      "Epoch 183/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0201\n",
      "Epoch 184/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0201\n",
      "Epoch 185/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 186/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 187/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 188/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 189/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0200\n",
      "Epoch 190/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0200\n",
      "Epoch 191/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 192/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 193/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0200\n",
      "Epoch 194/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0200\n",
      "Epoch 195/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0199\n",
      "Epoch 196/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0200\n",
      "Epoch 197/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0200\n",
      "Epoch 198/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0199\n",
      "Epoch 199/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0199\n",
      "Epoch 200/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0199\n",
      "Epoch 201/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0199\n",
      "Epoch 202/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0199\n",
      "Epoch 203/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0199\n",
      "Epoch 204/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0199\n",
      "Epoch 205/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0198\n",
      "Epoch 206/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 207/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 208/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 209/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 210/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 211/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 212/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 213/300\n",
      "105/105 [==============================] - 0s 161us/step - loss: 0.0197\n",
      "Epoch 214/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0198\n",
      "Epoch 215/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 216/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0198\n",
      "Epoch 217/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 218/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 219/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 220/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 221/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0198\n",
      "Epoch 222/300\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0198\n",
      "Epoch 223/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0198\n",
      "Epoch 224/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 225/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0197\n",
      "Epoch 226/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 227/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0198\n",
      "Epoch 228/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0197\n",
      "Epoch 229/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0197\n",
      "Epoch 230/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0196\n",
      "Epoch 231/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0196\n",
      "Epoch 232/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0197\n",
      "Epoch 233/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0197\n",
      "Epoch 234/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0198\n",
      "Epoch 235/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0197\n",
      "Epoch 236/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0196\n",
      "Epoch 237/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0196\n",
      "Epoch 238/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0196\n",
      "Epoch 239/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0196\n",
      "Epoch 240/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0196\n",
      "Epoch 241/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0196\n",
      "Epoch 242/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 243/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0195\n",
      "Epoch 244/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 245/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 246/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 247/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 248/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 249/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 250/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0195\n",
      "Epoch 251/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 252/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 253/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 254/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 255/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0196\n",
      "Epoch 256/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 257/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 258/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 259/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 260/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0195\n",
      "Epoch 261/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 262/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 263/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 264/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0194\n",
      "Epoch 265/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 266/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0194\n",
      "Epoch 267/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 268/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 269/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 270/300\n",
      "105/105 [==============================] - 0s 143us/step - loss: 0.0194\n",
      "Epoch 271/300\n",
      "105/105 [==============================] - 0s 124us/step - loss: 0.0194\n",
      "Epoch 272/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0194\n",
      "Epoch 273/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 274/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 275/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0193\n",
      "Epoch 276/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0193\n",
      "Epoch 277/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 278/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 279/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0193\n",
      "Epoch 280/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 281/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0194\n",
      "Epoch 282/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 283/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 284/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 285/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0193\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 287/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 288/300\n",
      "105/105 [==============================] - 0s 123us/step - loss: 0.0192\n",
      "Epoch 289/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0192\n",
      "Epoch 290/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 291/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0192\n",
      "Epoch 292/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 293/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 294/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0192\n",
      "Epoch 295/300\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.0192\n",
      "Epoch 296/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0192\n",
      "Epoch 297/300\n",
      "105/105 [==============================] - 0s 142us/step - loss: 0.0192\n",
      "Epoch 298/300\n",
      "105/105 [==============================] - 0s 132us/step - loss: 0.0192\n",
      "Epoch 299/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n",
      "Epoch 300/300\n",
      "105/105 [==============================] - 0s 133us/step - loss: 0.0192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6f299fa90>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, activation='relu', input_shape=(n_steps_past, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   64449],\n",
       "       [ 1806453],\n",
       "       [  846430],\n",
       "       [  124498],\n",
       "       [  356123],\n",
       "       [26184844],\n",
       "       [26145772],\n",
       "       [  141569],\n",
       "       [58903494],\n",
       "       [74932830],\n",
       "       [37295621],\n",
       "       [   43100]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test data\n",
    "test = test.reshape(-1, 1)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.38306737e-05],\n",
       "       [3.63110709e-03],\n",
       "       [1.67068267e-03],\n",
       "       [1.96454325e-04],\n",
       "       [6.69446437e-04],\n",
       "       [5.34132404e-02],\n",
       "       [5.33334531e-02],\n",
       "       [2.31314329e-04],\n",
       "       [1.20226682e-01],\n",
       "       [1.52959545e-01],\n",
       "       [7.61021120e-02],\n",
       "       [3.02347381e-05]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.38306737e-05],\n",
       "        [3.63110709e-03],\n",
       "        [1.67068267e-03]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_scaled[0:3].reshape((1, n_steps_past, n_features))\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test[1:3])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((1, n_steps_past, n_features))\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01113168]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(x_test, verbose=0)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original = scaler.inverse_transform(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5479494.5]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
